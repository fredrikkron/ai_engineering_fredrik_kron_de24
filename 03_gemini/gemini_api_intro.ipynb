{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf374de",
   "metadata": {},
   "source": [
    "# Gemini API Intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd04e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns from data to make smart decisions.\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# looks automatically after the key\n",
    "# one of GOOGLE_API_KEY and GOOGLE_API_KEY\n",
    "client = genai.Client()\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a69dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 data engineering jokes, structured in short points:\n",
      "\n",
      "*   What's a data engineer's favorite type of data? The kind that *doesn't* change its schema without notice.\n",
      "\n",
      "*   A business user says, \"Can you just pull the data?\" The data engineer replies, \"Sure, right after I build a new ingestion pipeline, schema migration, and a Spark cluster.\"\n",
      "\n",
      "*   How do you know a data engineer is stressed? They start muttering about \"one small change\" breaking an entire production pipeline.\n",
      "\n",
      "*   A data engineer’s motto: \"Our pipelines are resilient, until the source system decides to change a timestamp column to a string.\"\n",
      "\n",
      "*   Why did the data engineer refuse to play hide-and-seek? Because good data is hard enough to find as it is.\n"
     ]
    }
   ],
   "source": [
    "def ask_gemini(prompt, model = \"gemini-2.5-flash\"):\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    ")\n",
    "    \n",
    "    return response\n",
    "\n",
    "response = ask_gemini(\"Give me 5 some data engineering jokes, structure it in short points\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3060bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# knows that GenerateContentResponse is a pydantic model\n",
    "# -> we can work with it in a OOP manner\n",
    "isinstance(response, BaseModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9431a46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['sdk_http_response', 'candidates', 'create_time', 'model_version', 'prompt_feedback', 'response_id', 'usage_metadata', 'automatic_function_calling_history', 'parsed'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(response).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2bf4c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemini-2.5-flash'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf2a38c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HttpResponse(\n",
       "  headers=<dict len=11>\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.sdk_http_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b92463e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Candidate(\n",
       "   content=Content(\n",
       "     parts=[\n",
       "       Part(\n",
       "         text=\"\"\"Here are 5 data engineering jokes, structured in short points:\n",
       " \n",
       " *   What's a data engineer's favorite type of data? The kind that *doesn't* change its schema without notice.\n",
       " \n",
       " *   A business user says, \"Can you just pull the data?\" The data engineer replies, \"Sure, right after I build a new ingestion pipeline, schema migration, and a Spark cluster.\"\n",
       " \n",
       " *   How do you know a data engineer is stressed? They start muttering about \"one small change\" breaking an entire production pipeline.\n",
       " \n",
       " *   A data engineer’s motto: \"Our pipelines are resilient, until the source system decides to change a timestamp column to a string.\"\n",
       " \n",
       " *   Why did the data engineer refuse to play hide-and-seek? Because good data is hard enough to find as it is.\"\"\"\n",
       "       ),\n",
       "     ],\n",
       "     role='model'\n",
       "   ),\n",
       "   finish_reason=<FinishReason.STOP: 'STOP'>,\n",
       "   index=0\n",
       " )]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664925f",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "- basic unit of text for LLMs\n",
    "- can be as short as one character or as long as one word\n",
    "\n",
    "- tokens used for billing\n",
    "\n",
    "Gemini free tier\n",
    "- Requests per minute (RPM): 10\n",
    "- Tokens per minute (TPM): 250 000\n",
    "- Requests per day (RPD): 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1164553c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=172,\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  thoughts_token_count=1390,\n",
       "  total_token_count=1577\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thinking is expensive\n",
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7193a0b9",
   "metadata": {},
   "source": [
    "## Thinking\n",
    "\n",
    "- hyperparameter to allocate more compute for complex tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a45f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "prompt = \"Give me 5 some data engineering jokes, structure it in short points\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        thinking_config=types.ThinkingConfig(thinking_budget=0)\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a4c8cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=233,\n",
       "  prompt_token_count=15,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=15\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=248\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f5ab16",
   "metadata": {},
   "source": [
    "## System instruction\n",
    "\n",
    "- hyperparameter to guide model behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b0ff7fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## OOP in a Nutshell\n",
      "\n",
      "OOP (Object-Oriented Programming) is a programming paradigm based on \"objects\", which contain data (attributes) and code (methods) to manipulate that data. Key principles:\n",
      "\n",
      "*   **Encapsulation:** Bundling data and methods that operate on that data within a class.\n",
      "*   **Inheritance:** Creating new classes (subclasses) based on existing classes (superclasses), inheriting their attributes and methods.\n",
      "*   **Polymorphism:**  The ability of objects of different classes to respond to the same method call in their own way.\n",
      "*   **Abstraction:** Hiding complex implementation details and exposing only essential features.\n",
      "\n",
      "## Dunder (Magic) Methods\n",
      "\n",
      "Dunder methods (also called magic methods) are special methods in Python that start and end with double underscores (e.g., `__init__`, `__str__`). They provide a way to implement operator overloading and customize object behavior.\n",
      "\n",
      "**Examples:**\n",
      "\n",
      "*   `__init__`:  Constructor (called when an object is created).\n",
      "*   `__str__`:  String representation of an object (for `print()`).\n",
      "*   `__repr__`:  Unambiguous string representation of an object.\n",
      "*   `__len__`:  Returns the length of an object (for `len()`).\n",
      "*   `__add__`:  Defines addition behavior (for `+`).\n",
      "*   `__eq__`: Defines equality behavior (for `==`).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "You are an expert in Python programming, you will always provide idiomatic code, i.e pythonic code.\n",
    "So when you see my code or my question, be very critical, but answer in a SHORT and CONCISE way. \n",
    "Also be constructive to help me improve.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "Explain OOP and dunder methods\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=prompt,\n",
    "    config=types.GenerateContentConfig(\n",
    "        system_instruction=system_instruction)\n",
    "    )\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35ae6921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenerateContentResponseUsageMetadata(\n",
       "  candidates_token_count=308,\n",
       "  candidates_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=308\n",
       "    ),\n",
       "  ],\n",
       "  prompt_token_count=68,\n",
       "  prompt_tokens_details=[\n",
       "    ModalityTokenCount(\n",
       "      modality=<MediaModality.TEXT: 'TEXT'>,\n",
       "      token_count=68\n",
       "    ),\n",
       "  ],\n",
       "  total_token_count=376\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata = response.usage_metadata\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4230c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metadata.candidates_token_count = 308\n",
      "metadata.prompt_token_count = 68\n",
      "metadata.total_token_count = 376\n"
     ]
    }
   ],
   "source": [
    "print(f\"{metadata.candidates_token_count = }\") # output\n",
    "print(f\"{metadata.prompt_token_count = }\") # input + system instruction\n",
    "print(f\"{metadata.total_token_count = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9807491b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 43)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompt.split()), len(system_instruction.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697312ff",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "- controls randomness of output -> 'creative'\n",
    "\n",
    "- hyperparameter that can be adjusted to influence the diversity and creativity of the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74b963b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sensing the distant rumble of a lawnmower and immediately darted into the overgrown rose bushes. Safe within the thorny embrace, it nibbled on a fallen petal, the sweet scent masking the fear that still lingered in its twitching whiskers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43ca3964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sensing the distant rumble of a lawnmower and immediately darted into the overgrown rose bushes. Safe within the thorny embrace, it nibbled on a fallen petal, the sweet scent masking the fear that still lingered in its twitching whiskers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=0\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cad06971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sensing danger lurking in the tall grass. It thumped its foot, warning the others before darting into the burrow, the last of its family safe from the approaching fox.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=2.0\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fd60e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gray rabbit twitched its nose, sniffing the air for danger, before darting into the shadowed burrow. Inside, a warmth radiated from the shared body heat of her litter, a promise of safety in the cold, harsh woods.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "story = \"write a 2 sentence story about a gray rabbit\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=story,\n",
    "    config=types.GenerateContentConfig(\n",
    "        temperature=2.0\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1faa4c",
   "metadata": {},
   "source": [
    "# Multimodal input\n",
    "\n",
    "input text and image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d495080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A cute, gray, fluffy rabbit is resting on a carpet while wearing a small, plush Swedish graduation cap (studentmössa) and a blue and yellow ribbon around its neck.\n"
     ]
    }
   ],
   "source": [
    "text_input = \"Describe this image shortly\"\n",
    "image_input = {\"mime_type\": \"image/png\", \"data\": open(\"bella.png\", \"rb\").read()}\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-pro\",\n",
    "    contents=dict(\n",
    "        parts=[dict(text = text_input), dict(inline_data = image_input)]\n",
    "    )\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-fredrik-kron-de24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
